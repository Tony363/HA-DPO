model:
  arch: mini_gpt4_dpo
  model_type: pretrain_vicuna0 #pretrain_llama2

  max_txt_len: 320 #TODO 160???
  end_sym: "###" #"</s>"
  prompt_path: "ha_dpo/models/minigpt4/prompts/sed_alignment.txt"
  prompt_template: '###Human: {} ###Assistant: ' #'[INST] {} [/INST] '
  ckpt: '/home/tony/HA-DPO/ha_dpo/models/minigpt4/prerained_minigpt4_7b.pth'
  # pretrained_minigpt4_llama2_7b.pth

datasets:
  cc_sbu_align:
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"

run:
  task: image_text_pretrain
  seed: 42
  # batch_size_train: 1
  # batch_size_eval: 1
  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True