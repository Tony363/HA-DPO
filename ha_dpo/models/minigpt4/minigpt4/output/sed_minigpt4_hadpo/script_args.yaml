!!python/object:__main__.ScriptArguments
auxilary: true
beta: 0.1
ccsbualign_data_path: ha_dpo/data/lubal_sed_training
cfg_path: ha_dpo/models/minigpt4/train_configs/minigpt4_llama2_stage3_dpo.yaml
ddp_find_unused_parameters: false
desc_train_data_path: ha_dpo/data/hadpo/minigpt4/desc_data.json
eval_steps: null
evaluation_strategy: 'no'
freeze_llama_proj: true
gamma: 0.5
gradient_accumulation_steps: 1
gradient_checkpointing: false
ignore_bias_buffers: false
learning_rate: 0.0001
log_freq: 1
logging_steps: 4
lora_alpha: 16
lora_dropout: 0.05
lora_r: 64
lora_target_modules:
- q_proj
- k_proj
- v_proj
lr_scheduler_type: cosine
max_grad_norm: 1.0
max_length: 1024
max_prompt_length: 512
max_steps: 1000
model_name_or_path: ../sft/results/final_checkpoint
num_train_epochs: -1
optimizer_type: paged_adamw_32bit
output_dir: ha_dpo/models/minigpt4/minigpt4/output/sed_minigpt4_hadpo
per_device_eval_batch_size: 1
per_device_train_batch_size: 1
pope_train_data_path: ha_dpo/data/hadpo/minigpt4/pope_data.json
report_to: wandb
run_name: minigpt4
save_steps: -1
seed: 42
vg_path: ha_dpo/data/VG
warmup_steps: 100
weight_decay: 0.05
